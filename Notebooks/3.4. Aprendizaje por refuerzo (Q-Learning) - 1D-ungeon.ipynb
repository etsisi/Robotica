{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"vertical-align:middle\">\n",
    "<img width=\"170\" style=\"float: right;\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "<h1>Aprendizaje por refuerzo 1: 1D-ungeon<a id=\"top\"></a></h1>\n",
    "<i><small>Fuentes: <a href=\"https://github.com/etsisi/Robotica\">github.com/etsisi/Robotica</a><br>Última actualización: 2023-10-21</i>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción\n",
    "\n",
    "En este notebook aprenderemos las bases de cómo implementar una política de decisión (_policy_) y qué es el algoritmo Q-Learning. Para ello usaremos un ejemplo básico de RPG; tan básico que sólo habrá que explorar... una mazmorra en una dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de este Notebook son los siguientes:\n",
    "\n",
    "- Sentar las bases de qué son las _policy_ y por qué son importantes en el balance de exploración/explotación.\n",
    "- Aprender cómo funciona el algoritmo de Q-Learning para la asignación de valor a posibles transiciones entre estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuración\n",
    "\n",
    "A continuación importaremos las librerías que se usarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema\n",
    "\n",
    "Somos un aventurero en una mazmorra muy simple, 8 celdas una detrás de otra como se ve en la imagen adjunta:\n",
    "\n",
    "![](images/1dungeon-1.png)\n",
    "\n",
    "Nuestro conocimiento de la mazmorra es el siguiente:\n",
    "\n",
    "1. Sabemos que la mazmorra tiene 8 habitaciones.\n",
    "2. Nuestras acciones son movernos a la derecha (una habitación cada vez) o a la izquierda (todas las habitaciones hasta el principio). Es lo que tiene visitar una mazmorra encantada.\n",
    "3. Un conjuro de confusión existente en las habitaciones nos afecta un 10% de las veces, haciendo que cambie nuestra orientación (es decir, a veces decidimos ir a la derecha y vamos a la izquierda y viceversa).\n",
    "3. Algunas habitaciones tienen tesoro.\n",
    "\n",
    "Lo que no sabemos de la mazmorra es que:\n",
    "\n",
    "1. La primera habitación tiene una recompensa de 1 moneda de oro.\n",
    "2. La última habitación tiene 10 monedas de oro.\n",
    "3. Las monedas se regeneran cada turno.\n",
    "\n",
    "Podemos representar el Proceso de Decisión de Markov (en realidad una pequeña porción, porque es relativamente grande) con el siguiente grafo:\n",
    "\n",
    "![](images/1dungeon-2.png)\n",
    "\n",
    "Sí, es un poco desparrame. Es una de las desventajas de este tipo de modelos, que está muy bien para casos muy sencillos, pero representar espacios no tan sencillos ya es un reto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS = 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions(enum.Enum):\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "    \n",
    "    def switch(self):\n",
    "        if self == Actions.LEFT:\n",
    "            return Actions.RIGHT\n",
    "        else:\n",
    "            return Actions.LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dungeon:\n",
    "    def __init__(self, *, rooms, p_confussion):\n",
    "        self.rooms = rooms\n",
    "        self.p_confussion = p_confussion\n",
    "        self.state = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        if random.random() < self.p_confussion:\n",
    "            action = action.switch()\n",
    "\n",
    "        match action:\n",
    "            case Actions.LEFT:\n",
    "                self.state = 0\n",
    "            case Actions.RIGHT:\n",
    "                if self.state < len(self.rooms) - 1:\n",
    "                    self.state += 1\n",
    "\n",
    "        return self.state, self.rooms[self.state]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera aproximación: El aventurero que no sabe si va o viene\n",
    "\n",
    "La primera aproximación será, simplemente, escoger ir a izquierda o a derecha aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hero:\n",
    "    def __init__(self, *, dungeon):\n",
    "        self.dungeon = dungeon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        return Actions.LEFT if random.random() < 0.5 else Actions.RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 578578\n"
     ]
    }
   ],
   "source": [
    "dungeon = Dungeon(rooms=[1, 0, 0, 0, 0, 0, 0, 10], p_confussion=0.1)\n",
    "hero = Hero(dungeon=dungeon)\n",
    "\n",
    "total = 0\n",
    "for i in range(TRAIN_STEPS):\n",
    "    old_state = dungeon.state\n",
    "    action = hero.select_action(old_state) \n",
    "    new_state, reward = dungeon.step(action)\n",
    "    total += reward\n",
    "\n",
    "print(f'Total reward: {total}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda aproximación: El aventurero enzarpado\n",
    "\n",
    "Nuestro segundo aventurero seguirá la siguiente estrategia de toma de decisiones:\n",
    "\n",
    "1. Si desde donde estoy hay una acción más lucrativa que otra, realizaré dicha acción.\n",
    "2. Si ambas son iguales, escogeré una aleatoria.\n",
    "\n",
    "Para ello, en su parte trasera del mapa irá apuntando qué acciones han sido más lucrativas que otras. Veamos una simulación de este comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hero:\n",
    "    def __init__(self, *, dungeon):\n",
    "        self.dungeon = dungeon\n",
    "        self.actions_values = {\n",
    "            Actions.LEFT: [0 for _ in dungeon.rooms],\n",
    "            Actions.RIGHT: [0 for _ in dungeon.rooms],\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if self.actions_values[Actions.RIGHT][state] < self.actions_values[Actions.LEFT][state]:\n",
    "            return Actions.LEFT\n",
    "        elif self.actions_values[Actions.LEFT][state] < self.actions_values[Actions.RIGHT][state]:\n",
    "            return Actions.RIGHT\n",
    "        else:\n",
    "            return Actions.LEFT if random.random() < 0.5 else Actions.RIGHT\n",
    "\n",
    "    def update(self, old_state, action, reward):\n",
    "        self.actions_values[action][old_state] += reward\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for k, v in self.actions_values.items():\n",
    "            string += f'{k}: {v}\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions.LEFT: [808682, 81659, 8059, 836, 82, 0, 9, 0]\n",
      "Actions.RIGHT: [0, 0, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "Total reward: 899328\n"
     ]
    }
   ],
   "source": [
    "dungeon = Dungeon(rooms=[1, 0, 0, 0, 0, 0, 0, 10], p_confussion=0.1)\n",
    "hero = Hero(dungeon=dungeon)\n",
    "\n",
    "total = 0\n",
    "for i in range(TRAIN_STEPS):\n",
    "    old_state = dungeon.state\n",
    "    action = hero.select_action(old_state) \n",
    "    new_state, reward = dungeon.step(action)\n",
    "    hero.update(old_state, action, reward)\n",
    "    total += reward\n",
    "\n",
    "print(hero)\n",
    "print(f'Total reward: {total}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tercera aproximación: Aventurero que sigue la estrategia $\\epsilon$-greedy\n",
    "\n",
    "En la anterior aproximación, el aventurero enzarpado **siempre** iba hacia la mejor recompensa que había encontrado en el pasado. Claro, lo más probable es ir cayendo en la recompensa pequeña por lo que prácticamente nunca irá hacia la derecha. Estamos prácticamente explotando y no explorando.\n",
    "\n",
    "La estrategia $\\epsilon$-greedy tratará de solventar esto. Es un método muy simple para equilibrar la exploración y la explotación aleatoriamente. El factor $\\epsilon \\in [0, 1]$ es el que se encargará de determinar cuál es la acción preferida, si explotar o explorar, y tiene la siguiente forma:\n",
    "\n",
    "$$p_\\epsilon = 1 - \\epsilon$$\n",
    "\n",
    "Cuando $\\epsilon$ está muy próximo a $1$, el comportamiento será de total exploración, es decir, un comportamiento aleatorio, y cuando está muy próximo a $0$, la acción que seguirá el agente es de total explotación.\n",
    "\n",
    "Nuestra implementación dará un paso más y usará la estrategia $\\epsilon$-greedy con decaimiento. Eso significa que el factor épsilon no es fijo, sino que a lo largo del entrenamiento irá disminuyendo, de tal manera que en los primeros estadios del proceso de entrenamiento tendrá un valor muy alto, favoreciendo la exploración, y según avance el entrenamiento irá decayendo favoreciendo la explotación. Esto es porque se espera que una vez avanzado el entrenamiento, el algoritmo ha sido capaz de encontrar regiones prometedoras con la exploración y ahora queremos explotarlas a ver hasta qué punto son buenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hero:\n",
    "    def __init__(self, *, dungeon, epsilon, d_epsilon):\n",
    "        self.dungeon = dungeon\n",
    "        self.epsilon = epsilon\n",
    "        self.d_epsilon = d_epsilon\n",
    "        self.actions_values = {\n",
    "            Actions.LEFT: [0 for _ in dungeon.rooms],\n",
    "            Actions.RIGHT: [0 for _ in dungeon.rooms],\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < 1 - self.epsilon:\n",
    "            if self.actions_values[Actions.RIGHT][state] < self.actions_values[Actions.LEFT][state]:\n",
    "                return Actions.LEFT\n",
    "            elif self.actions_values[Actions.LEFT][state] < self.actions_values[Actions.RIGHT][state]:\n",
    "                return Actions.RIGHT\n",
    "        return Actions.LEFT if random.random() < 0.5 else Actions.RIGHT\n",
    "\n",
    "    def update(self, old_state, action, reward):\n",
    "        self.actions_values[action][old_state] += reward\n",
    "\n",
    "        if self.epsilon > 0:\n",
    "            self.epsilon -= self.d_epsilon\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for k, v in self.actions_values.items():\n",
    "            string += f'{k}: {v}\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions.LEFT: [647676, 104246, 23195, 7158, 2628, 1091, 634, 787]\n",
      "Actions.RIGHT: [7763, 2753, 1074, 445, 169, 70, 4295, 5636]\n",
      "\n",
      "Total reward: 809620\n"
     ]
    }
   ],
   "source": [
    "dungeon = Dungeon(rooms=[1, 0, 0, 0, 0, 0, 0, 10], p_confussion=0.1)\n",
    "hero = Hero(dungeon=dungeon, epsilon=1.0, d_epsilon=1 / (TRAIN_STEPS / 2))\n",
    "\n",
    "total = 0\n",
    "for i in range(TRAIN_STEPS):\n",
    "    old_state = dungeon.state\n",
    "    action = hero.select_action(old_state)\n",
    "    new_state, reward = dungeon.step(action)\n",
    "    hero.update(old_state, action, reward)\n",
    "    total += reward\n",
    "\n",
    "print(hero)\n",
    "print(f'Total reward: {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuarta aproximación: Aventurero que sigue la estrategia $\\epsilon$-greedy y que de verdad sabe lo que valen las cosas\n",
    "\n",
    "En al anterior aproximación hemos usado una estrategia distinta, y generalmente no es tan buena como la estrategia del enzarpado. Esto es porque la información que el agente guarda en su memoria es el de las recompensas inmediatas.\n",
    "\n",
    "La idea ahora será usar $q$-learning. Con esta estrategia de valoración de acciones y estados no almacenaremos el valor o la recompensa inmediata, sino que guardaremos también cierta memoria de acciones pasadas que nos irán indicando que a la derecha, a veces, hemos encontrado una región más prometedoras. Al variar la tabla de valores, la estrategia $\\epsilon$-greedy tendrá un escenario sobre el que trabajar más útil durante la exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hero:\n",
    "    def __init__(self, *, dungeon, epsilon, d_epsilon, alpha, gamma):\n",
    "        self.dungeon = dungeon\n",
    "        self.epsilon = epsilon\n",
    "        self.d_epsilon = d_epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.q_table = {\n",
    "            Actions.LEFT: [0 for _ in dungeon.rooms],\n",
    "            Actions.RIGHT: [0 for _ in dungeon.rooms],\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < 1 - self.epsilon:\n",
    "            return self._greedy(state)\n",
    "        else:\n",
    "            return self._random(state)\n",
    "\n",
    "    def _greedy(self, state):\n",
    "        if self.q_table[Actions.RIGHT][state] < self.q_table[Actions.LEFT][state]:\n",
    "            return Actions.LEFT\n",
    "        elif self.q_table[Actions.LEFT][state] < self.q_table[Actions.RIGHT][state]:\n",
    "            return Actions.RIGHT\n",
    "        else:\n",
    "            return self._random(state)\n",
    "\n",
    "    def _random(self, state):\n",
    "        return Actions.LEFT if random.random() < 0.5 else Actions.RIGHT\n",
    "\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        old_value = self.q_table[action][old_state]\n",
    "\n",
    "        future_action = self._greedy(new_state)\n",
    "        future_reward = self.q_table[future_action][new_state]\n",
    "\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * future_reward - old_value)\n",
    "        self.q_table[action][old_state] = new_value\n",
    "\n",
    "        if self.epsilon > 0:\n",
    "            self.epsilon -= self.d_epsilon\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for k, v in self.q_table.items():\n",
    "            string += f'{k}: {v}\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions.LEFT: [63.23435499738744, 64.62675154943936, 66.26506470448018, 66.40252655500417, 65.84793948478094, 70.67595935412251, 65.96732938111782, 68.11611400084047]\n",
      "Actions.RIGHT: [63.80953683283365, 67.06261213529453, 71.30247397659673, 75.63065716624668, 80.24656937523632, 86.62800999326903, 97.61939855595003, 110.47327151056649]\n",
      "\n",
      "Total reward: 1636764\n"
     ]
    }
   ],
   "source": [
    "dungeon = Dungeon(rooms=[1, 0, 0, 0, 0, 0, 0, 10], p_confussion=0.1)\n",
    "hero = Hero(dungeon=dungeon, epsilon=1.0, d_epsilon=1 / TRAIN_STEPS, alpha=0.1, gamma=0.95)\n",
    "\n",
    "total = 0\n",
    "for i in range(TRAIN_STEPS):\n",
    "    old_state = dungeon.state\n",
    "    action = hero.select_action(old_state) \n",
    "    new_state, reward = dungeon.step(action)\n",
    "    hero.update(old_state, new_state, action, reward)\n",
    "    total += reward\n",
    "print(hero)\n",
    "print(f'Total reward: {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep $Q$-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Any\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    prev_state: Any\n",
    "    next_state: Any\n",
    "    action: Any\n",
    "    reward: float\n",
    "    terminated: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class Hero:\n",
    "    def __init__(self, *, dungeon, epsilon, d_epsilon, gamma):\n",
    "        self.dungeon = dungeon\n",
    "        self.epsilon = epsilon\n",
    "        self.d_epsilon = d_epsilon\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.model = Sequential([\n",
    "            Dense(4, activation=\"tanh\", input_shape=(,1)),\n",
    "            Dense(1, activation=\"sigmoid\"),\n",
    "        ])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < 1 - self.epsilon:\n",
    "            return self._greedy(state)\n",
    "        else:\n",
    "            return self._random(state)\n",
    "\n",
    "    def _greedy(self, state):\n",
    "        self.model.predict(np.array(transition.\n",
    "\n",
    "        \n",
    "        if self.q_table[Actions.RIGHT][state] < self.q_table[Actions.LEFT][state]:\n",
    "            return Actions.LEFT\n",
    "        elif self.q_table[Actions.LEFT][state] < self.q_table[Actions.RIGHT][state]:\n",
    "            return Actions.RIGHT\n",
    "        else:\n",
    "            return self._random(state)\n",
    "\n",
    "    def _random(self, state):\n",
    "        return Actions.LEFT if random.random() < 0.5 else Actions.RIGHT\n",
    "\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):\n",
    "        old_value = self.q_table[action][old_state]\n",
    "\n",
    "        future_action = self._greedy(new_state)\n",
    "        future_reward = self.q_table[future_action][new_state]\n",
    "\n",
    "        new_value = old_value + self.alpha * (reward + self.gamma * future_reward - old_value)\n",
    "        self.q_table[action][old_state] = new_value\n",
    "\n",
    "        if self.epsilon > 0:\n",
    "            self.epsilon -= self.d_epsilon\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        for k, v in self.q_table.items():\n",
    "            string += f'{k}: {v}\\n'\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumiendo\n",
    "\n",
    "Aunque es un algoritmo muy básico, podemos ver que un algoritmo de aprendizaje de mapeo estado-acción de tipo $Q$-learning es altamente efectivo en problemas de tiempo discreto y estados (y acciones) discretas. No obstante, uno de los problemas de este tipo de algoritmos es el de la explosión combinatoria, y se vuelven bastante inefectivos (por lo costoso que es computacionalmente) cuando los espacios de estados y acciones son muy grandes.\n",
    "\n",
    "Esos casos se resuelven con técnicas de reducción de la dimensionalidad, deep $q$-learning o similares, que se nos escapan a esta introducción. Lo que no quiere decir que no lo miréis si os interesa. Sentíos libres de explorar y compartir lo que aprendáis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<img style=\"float: right;\" src=\"data:image/png;base64,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\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
